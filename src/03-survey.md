## What Neural Networks Are and Are Not

The very term "neural network" invokes cybernetic imagery. We easily fall into
the old sci-fi fantasy of thinking, conscious machinery. While seductive, this
account of what a neural network is doing is practically entirely false. Neural
networks are structurally and functionally dissimilar from real brains, even if
the brain's neural connectivity inspired the computational infrastructure.

![Neural networks are inspired by the brain, but it is important not to think of them as actual brains.](figures/03/brain-01.png)

In this section, we'll develop intuition around what tasks neural networks
succeed at and debunk the mythology around these "artificial brains." With this
knowledge, we can evaluate why they aren't quite brains, but still do an
impressive job of modeling highly complex functions that can classify large and
diverse data -- and, importantly, learn how to identify when claims around
neural networks are being overstated.

### Interpreting a Neural Network

In Section 2, we covered the basic architecture and computational techniques
used to design and train neural networks. How computer scientists often
interpret these systems is that we are creating one glorified _feature extractor_.
That is, we comprehend our data as having discrete features that, once teased
out, tell us how that data should be treated given some classification or
prediction goal. This is the assumption of _separability_ that essentially means
we believe our data's features can be identified, separated, and bounded so as
to make robust class distinctions. Neural networks are tools for extracting out
these features and, in turn, making classification decisions.

You may remember that with convolutional neural networks (which are used for object
recognition), we use _kernels_ to encode features from smaller
sections of our images (e.g., detecting an edge). With these features encoded,
we can then use further layers to separate our feature space. And this is the
essence of even the most complicated, multilayered neural networks: layer by
layer we are transforming our data, first to encode the data's features and then to
filter them based on how our model has "learned" to categorize those features.
Because these transformations are nonlinear and we do not
actually know what features each layer is encoding, any further interpretation
requires a very fine, technical inspection of the model in action.

Due to this complexity, many researchers have worked on visual techniques that
give them insight into what is happening at each layer. ^[See e.g.
[http://arxiv.org/abs/1506.02078](http://arxiv.org/abs/1506.02078) or [http://arxiv.org/abs/1412.0035](http://arxiv.org/abs/1412.0035).] There are
many people who specialize in gaining practical results (e.g., tuning
hyperparameters, adding/subtracting layers), but this high-level description is
about as close as we get to interpreting what these systems are actually doing.

Some of you may have seen the images generated by Google's Deep Dream software,
which incepts images with psychedelic-looking dogs, buildings, etc.
^[[https://github.com/google/deepdream](https://github.com/google/deepdream)] This technique was designed to
gain insight into what the neural network is encoding at a particular layer.
The tactic is to take an already trained network but only evaluate it up to the
layer we want to inspect. Then, taking a separate image that we want to "incept"
into our neural network, we run our inception image up to the layer we want to
inspect and get its values. Then we run our backpropagation algorithm, except
instead of minimizing our usual cost function, we minimize against the values
found for the incepted image. Finally, we backpropagate all the way into the
image, treating it as a modifiable layer rather than fixed input data, adjusting
its pixel values to get a visual representation of what it's trying to encode.

![A human face "incepted" with a cat's using Deep Dream.](figures/03/deep-dream-01.png)

While this method may seem like a toy and nothing else, it provided us with the
first insight into the complexity of the features that a convolutional neural
network was using to do its classification. Before, the only way to see what
features were being extracted was to look at the activation maps (see
<<activationmap>>); however, this does not give us a good holistic view of
what is happening (in fact, it takes quite a bit of expertise to understand
anything that is going on in an activation map). But by taking an image
and forcing the neural network to see cats inside of it, we can inspect each
layer of the network and gain some insight regarding what shapes, colors, and
general features the model is expecting for such a classification.

![Resulting images from deepdream by setting the guide image at different layer depths in the neural network.  Deeper layers contain more complex and abstract features.](figures/03/inception-01.png)

Google's method is a taste of how one may be creative in figuring out what a
neural network is encoding. It is only good for visual images, and tells us a
limited amount, but it allows us a visual -- and sometimes creepy -- insight
into what the weights and biases of a particular layer _mean_.

### Limitations

For someone who makes plans and decisions based on emerging technologies, it is
crucial to have a discerning eye for what is achievable and what is far-fetched.
Having now seen the basic concepts behind neural networks and a functional
interpretation, you may already have some inkling as to why these systems are
not "strong AI."
^[[https://en.wikipedia.org/wiki/Artificial_general_intelligence](https://en.wikipedia.org/wiki/Artificial_general_intelligence)] While
neural networks are powerful, the brain-like vocabulary is misleading.
Obviously, they are materially different, but these systems are not "brains"
from a functional standpoint either.

To begin with, brains have plastic connections -- neuron A may not _always_ be
connected to neuron B at all times -- whereas in neural networks these
connections are rigidly defined. Further, in brains, everything is a function:
even activation functions are functions since neuronal activations are primed by
many conditions determined by the local chemical state. Neural inhibitors and
transmitters modify individual synapses, changing both the firing potential and
the signals transmitted. Rather than accepting input vectors of a specific
shape, brains adapt to multimodal inputs that are combined internally.

All of these functional differences must be taken seriously before we go too far
in comparing neural networks to human intelligence. These descriptions tell us
what's different, yet only go so far in characterizing what they amount to in
the way of limitations. Many of the tasks that we are eager to see AI accomplish
involve cognitive criteria for success. Understanding this distinction between
cognitive and computational involves clarifying the difference between
learning and thinking, as well as processing and integrating information.

As was elaborated in Section 2, neural networks have a capacity for learning via
parameter adjustment. Recall that this learning is the result of minimizing
a single objective function defining our task. The existence of the
objective function makes learning in neural networks quite different from in
brains. Why we will distinguish the role of _thinking_ to understand this contrast is
because the process of thinking involves switching and integrating contexts --
something neural networks are not yet able to do. Here, learning is the feedback
system that improves practical results on a task, whereas thinking is the
process that reflects on the task, our goal orientation toward it, the varying
approaches we may take, etc. In neural network terms, this would imply objective
functions constantly morphing (task redefinition) and the shape of input vectors
changing (context adjustment) as needed. Consider being taught a word, but then
asking your teacher for a visual demonstration to go along with the syntactic
definition -- both the visual and auditory contexts aid in your learning. Neural
networks do not have this flexibility.

An implication of how neural networks are currently designed is that they are
processing systems rather than integrating systems. That is, they do not know or
care about outside information that may be pertinent to their task, or about whether
their goal is appropriate. They merely act on particular input shapes, achieving
particular output shapes using rigidly defined computations. An integrating
system requires a higher-order functioning that cannot be reduced to any
particular processing task, which itself can include/exclude specific
information and adjust the task. It is integration that is needed for cognitive
tasks and that must be worked on before AI encroaches on the kind of intelligence we
dream about in our sci-fi fantasies -- strong AI.

With that said, we still find neural networks extremely useful because they
allow us to take a diversity of inputs and model arbitrarily complex functions.
Given that developing functions for mapping intricate inputs into simple outputs
is an extremely difficult task, neural networks offer technologists an
incredible service by being able to approximate any continuous function capable
of being written down. We call this the _universality_ of neural networks, and
it will be touched on in the next section.

### Common Misconceptions

Despite the limitations characterized above, neural networks continue to be in
the press with many fantastical articles written about them. It seems that
every week a new neural network is devised that reportedly gets closer and closer to being
able to read your thoughts or interact with you in a deeply personal way.
However, as you now know, these claims are far from the truth. While neural
networks are indeed advancing, we are at a point where either
vast improvements must be made to the algorithms or we must devise
exponentially bigger computers. ^[Further reading on this topic can be
found at https://timdettmers.wordpress.com/2015/07/27/brain-vs-deep-learning-singularity/]

This is because current neural networks are quite limited in the generality they
can obtain. We are only now reaching levels where image classification can be
done at an accuracy better than that achievable by humans; however, that is only
for a very focused set of classification labels on a very curated dataset (known
as the ImageNet dataset, which is used annually in image recognition
competitions).

It has been shown, however, that neural networks do very well on transfer tasks,
where the model has been trained on one problem and is then applied to a different
problem with minimal retraining. However, just as before, these tasks must
have quantifiable objectives, making them very limited in scope -- a limitation
currently affected by the sizes of both the models and the datasets we have
available to train on.

Furthermore, an aspect of neural networks that is often left undiscussed in
public is how the exact formulation of the data determines our effectiveness.
We may feel tempted to believe that if we had a large enough neural network and
all of Facebook's data, we could predict users' preferences given certain online
actions. This misconception stems from the nature of training on cost functions,
since neural networks currently only show their real utility for supervised
learning problems where the correct results are known for the given training
set. That is, to train the neural network we must have a labeled dataset that
teaches the model correct relationships between input and desired output.

This problem with datasets can also lead to many subtleties where a dataset does
indeed exist, but is not robust enough to fully describe the problem. For
example, if we had images of news anchors as they were talking about different
stories, and each image was labeled with the topic of the story the person was
discussing, would we be able to later determine the topic of a story given an
image of the news anchor? This might be possible in certain cases -- for
example, if the topic was weather we could probably use the existence of a
weather map in the background as a giveaway. However, for many other topics
there are no cues that could be used to determine what is being talked about,
since we are simply using the wrong data -- we are using images of the anchor
when what we really want is audio or a transcription of what is being discussed!
This type of problem is quite prevalent, where it seems our dataset has the
right association, but it is missing the correct context in order to find a
reasonable solution to the problem.

In the end, neural networks are not magical devices that can solve any problem
they are given. Instead, they are a way of training incredibly large models to
solve very high dimensional problems. So as you consider whether or not they are
right given your problem, or whether to believe someone else's claim to be
solving a problem using a neural network, here's a list of questions to ask
yourself:

* Can you quantitatively describe the features of your problem space?
* Does the problem always require the same information to solve?
* If you were to give this task to a group of humans, would you expect consensus?
* Is the problem's solution context dependent? If so, does the data cover that context?
* Can you verify the correctness of a given output?
* Does the neural network claim to know you better than you know yourself?

